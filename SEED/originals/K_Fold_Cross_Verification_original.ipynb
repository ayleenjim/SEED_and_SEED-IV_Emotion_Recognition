{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHTVjRvXRWDt",
        "outputId": "a64d94b0-3c32-4fbb-a286-1cdc3e352dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (1.4.7)\n",
            "Requirement already satisfied: keras in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: kt-legacy in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: requests in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from packaging->keras-tuner) (3.1.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from requests->keras-tuner) (1.26.20)\n",
            "Requirement already satisfied: mat73 in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (0.65)\n",
            "Requirement already satisfied: h5py in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from mat73) (3.1.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from mat73) (1.19.5)\n",
            "Requirement already satisfied: cached-property in c:\\users\\drl782\\.conda\\envs\\ppbemo\\lib\\site-packages (from h5py->mat73) (1.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner\n",
        "!pip install mat73\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import mat73\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn.metrics\n",
        "from sklearn.utils import shuffle \n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Conv3D, MaxPooling3D, BatchNormalization\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGX33OULSUjR",
        "outputId": "12d04523-2b7f-42d0-ff5d-69d8b9e87ee9"
      },
      "outputs": [],
      "source": [
        "mat_file = sio.loadmat('ExtractedFeatures_1s/data.mat')\n",
        "mat_file_labels = sio.loadmat('ExtractedFeatures_1s/label.mat')\n",
        "count = 0\n",
        "accuracy = []\n",
        "\n",
        "data = mat_file['data']\n",
        "labels = mat_file_labels['label']\n",
        "\n",
        "labels = labels[0]\n",
        "labels_edited = np.empty(675)\n",
        "for i in range(0,45):\n",
        "  labels_edited[i*15:(i+1)*15] = labels\n",
        "\n",
        "data, labels_edited = shuffle(data, labels_edited)\n",
        "\n",
        "max = np.max(data)\n",
        "min = np.min(data)\n",
        "\n",
        "#Normalizing data\n",
        "data = data/max\n",
        "data = (data - np.mean(data))/np.std(data)\n",
        "\n",
        "#Splitting Dataset into train, validation, test \n",
        "train_labels = labels_edited[0:550]\n",
        "test_labels = labels_edited[550:600]\n",
        "train_data = data[0:550]\n",
        "test_data = data[550:600]\n",
        "final_test = data[600:675]\n",
        "final_labels = labels_edited[600:675]\n",
        "cf_labels = np.where(final_labels== -1, 2, final_labels) #Label in the form for confusion matrix\n",
        "un, co = np.unique(cf_labels, return_counts=True)\n",
        "print(f'Unique: {un}, Counts: {co}')\n",
        "\n",
        "train_labels_reshaped = train_labels.reshape(-1,1) #Formatting for input to the CNN model\n",
        "test_labels_reshaped = test_labels.reshape(-1,1)\n",
        "final_labels_reshaped = final_labels.reshape(-1,1)\n",
        "\n",
        "train_labels_reshaped = to_categorical(train_labels_reshaped, 3) #One Hot Encoding\n",
        "test_labels_reshaped = to_categorical(test_labels_reshaped, 3)\n",
        "final_labels_reshaped = to_categorical(final_labels_reshaped, 3)\n",
        "\n",
        "rnn_train = train_data.reshape(550, 62, -1) #Formatting for CNN input\n",
        "rnn_test = test_data.reshape(50, 62, -1)\n",
        "rnn_train = np.transpose(rnn_train, (0,2,1))\n",
        "rnn_test = np.transpose(rnn_test, (0,2,1))\n",
        "\n",
        "final_rnn_test = final_test.reshape(75, 62, -1)\n",
        "final_rnn_test = np.transpose(final_rnn_test, (0,2,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEF1W651QSHX"
      },
      "outputs": [],
      "source": [
        "#K Fold Cross Validation Test\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "cvscores = []; cvscores_rnn = []; cvscores_hybrid = []; cvscores_ensemble = []\n",
        "for train, test in kfold.split(data, labels_edited):\n",
        "  # CNN Model\n",
        "  model = Sequential([\n",
        "    Conv2D(filters= 64, kernel_size= 5, input_shape = (62,265,5), padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2,strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.3),\n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2, strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.2),\n",
        "\n",
        "    Conv2D(filters= 256, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    MaxPooling2D(pool_size= 2, strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.25),\n",
        "\n",
        "    Conv2D(filters= 512, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2, strides= 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.3),\n",
        "    \n",
        "    Flatten(),\n",
        "\n",
        "    Dense(512, activation= 'relu'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.4),\n",
        "    Dense(256, activation= 'relu'),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(rate= 0.2),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    #BatchNormalization(),\n",
        "    Dense(3, activation= 'softmax')\n",
        "  ])\n",
        "\n",
        "  #Compile CNN Model\n",
        "  model.compile(optimizer= keras.optimizers.Adam(learning_rate=9e-5), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])  \n",
        "  \n",
        "  #Fit the CNN model\n",
        "  reduce_lr_cnn = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-7) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history = model.fit(\n",
        "    train_data,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 64,\n",
        "    epochs=100, \n",
        "    validation_data=(test_data, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_cnn]\n",
        "  )\n",
        "\n",
        "  \n",
        "  \n",
        "  #LSTM model \n",
        "  model_rnn = Sequential([\n",
        "    LSTM(units= 64, activation= 'tanh', input_shape= [1325, 62], return_sequences = True),\n",
        "    Dropout(0.25),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.35),\n",
        "    LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(256, activation= 'relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(128, activation= 'relu'),\n",
        "    Dropout(0.35),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    Dense(3, activation= 'softmax'),\n",
        "\n",
        "  ])\n",
        "\n",
        "  #Compile LSTM Model\n",
        "  model_rnn.compile(optimizer= keras.optimizers.Adam(learning_rate=5e-5), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "  \n",
        "  #Fit LSTM Model\n",
        "  reduce_lr_rnn = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history_rnn = model_rnn.fit(\n",
        "    rnn_train,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 16,\n",
        "    epochs=60,\n",
        "    validation_data=(rnn_test, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_rnn]\n",
        "  )\n",
        "  \n",
        "  #Hybrid model \n",
        "  model_hybrid = Sequential([\n",
        "    Conv2D(filters= 64, kernel_size= 5, input_shape = (62,265,5), padding= 'same'), \n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2,strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'), \n",
        "    Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "    MaxPooling2D(pool_size= 2, strides = 2),\n",
        "    #BatchNormalization(),\n",
        "    Dropout(0.35),\n",
        "    \n",
        "    Conv2D(filters= 256, kernel_size= 3, padding= 'same'), \n",
        "    #MaxPooling2D(pool_size= 2, strides = 2), # This wasn't here, all batch norm\n",
        "    #BatchNormalization(),\n",
        "\n",
        "    Reshape((66, 15*256), input_shape= (15, 66, 256)), #Important to reshape so data passed to LSTM Layer correctly\n",
        "    \n",
        "    LSTM(units= 64, activation= 'tanh', input_shape= [1325, 62], return_sequences = True),\n",
        "    Dropout(0.25),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "    Dropout(0.35),\n",
        "    LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "    \n",
        "    Flatten(),\n",
        "    \n",
        "    Dense(512, activation= 'relu'), \n",
        "    Dropout(0.25), \n",
        "    Dense(256, activation= 'relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation= 'relu'),\n",
        "    Dense(3, activation= 'softmax')\n",
        "  ])\n",
        "\n",
        "  #Compile Hybrid Model \n",
        "  model_hybrid.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  #Fit the Hybrid Model\n",
        "  reduce_lr_hybrid = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history_hybrid = model_hybrid.fit(\n",
        "    train_data,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 32,\n",
        "    epochs=60,\n",
        "    validation_data=(test_data, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_hybrid]\n",
        "  )\n",
        "  \n",
        "  #Evaluate and print on every run\n",
        "  scores = model.evaluate(x= final_test, y = final_labels_reshaped)\n",
        "  scores_rnn = model_rnn.evaluate(x= final_rnn_test, y = final_labels_reshaped)\n",
        "  scores_hybrid = model_hybrid.evaluate(x= final_test, y = final_labels_reshaped)\n",
        "\n",
        "  #Implementing Ensemble (Stacking method) using a meta model \n",
        "\n",
        "  #Generating Predictions\n",
        "  pred = np.argmax(model.predict(train_data), axis=-1)\n",
        "  pred_rnn = np.argmax(model_rnn.predict(rnn_train), axis=-1)\n",
        "  pred_hybrid = np.argmax(model_hybrid.predict(train_data), axis=-1)\n",
        "  inputs = [pred, pred_rnn, pred_hybrid]\n",
        "  inputs = np.array(inputs).T\n",
        "\n",
        "  pred_test = np.argmax(model.predict(test_data), axis=-1)\n",
        "  pred_rnn_test = np.argmax(model_rnn.predict(rnn_test), axis=-1)\n",
        "  pred_hybrid_test = np.argmax(model_hybrid.predict(test_data), axis=-1)\n",
        "  inputs_test = [pred_test, pred_rnn_test, pred_hybrid_test]\n",
        "  inputs_test = np.array(inputs_test).T\n",
        "\n",
        "  #Meta Model\n",
        "  model_stack = Sequential([\n",
        "    Dense(128, 'relu', input_shape= (3,)),\n",
        "    Dense(256, 'relu'),\n",
        "    Dense(256, 'relu'),\n",
        "    Dense(64, 'relu'),\n",
        "    Dense(3, 'softmax')\n",
        "  ])\n",
        "\n",
        "  #Compiling the Meta Model\n",
        "  model_stack.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= 'accuracy')\n",
        "\n",
        "  #Fitting the Meta Model\n",
        "  reduce_lr_stack = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "  history_stack = model_stack.fit(\n",
        "    inputs,\n",
        "    train_labels_reshaped,\n",
        "    batch_size = 32,\n",
        "    epochs=150,\n",
        "    validation_data=(inputs_test, test_labels_reshaped),\n",
        "    callbacks = [reduce_lr_stack]\n",
        "  )\n",
        "\n",
        "  #Evaluation, Print and store Results\n",
        "  acc = model_stack.evaluate(x= final_preds.T, y= final_labels_reshaped)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  print(\"%s: %.2f%%\" % (model_rnn.metrics_names[1], scores_rnn[1]*100))\n",
        "  print(\"%s: %.2f%%\" % (model_hybrid.metrics_names[1], scores_hybrid[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\n",
        "  cvscores_rnn.append(scores_rnn[1] * 100)\n",
        "  cvscores_hybrid.append(scores_hybrid[1] * 100)\n",
        "  cvscores_ensemble.append(acc[1])\n",
        "\n",
        "\n",
        "  print(\"%s: %.2f%%\" % (model_hybrid.metrics_names[1], scores_hybrid[1]*100))\n",
        "  cvscores_hybrid.append(scores_hybrid[1] * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDWVwPHibSa1"
      },
      "outputs": [],
      "source": [
        "#Print results and Box and Whisker Plot for Hybrid model. Can be changed for each model.\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Hybrid K Fold Validation Test Boxplot (K=10)')\n",
        "ax.boxplot((cvscores_hybrid), showfliers=True)\n",
        "print(f'Accuracy Average: {np.mean(cvscores_hybrid)}, Standard Deviation: {np.std(cvscores_hybrid)}, Max: {np.max(cvscores_hybrid)}, Min: {np.min(cvscores_hybrid)}')\n",
        "\n",
        "#Uncomment to save results and box and whisker plots\n",
        "#np.savetxt('hybrid_accuracies.csv', cvscores_hybrid, delimiter=',')\n",
        "#plt.savefig('Hybrid_boxplot.jpg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "K Fold Verification",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ppbEmo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
