{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrAwmpz3ZjB"
      },
      "source": [
        "# EEG Classification Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyIT6Er83eqi"
      },
      "source": [
        "## Imports and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5ZWr1tO4rn",
        "outputId": "b947241f-4b72-4ab4-807d-45bd91dbab98"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner\n",
        "!pip install mat73\n",
        "!pip install -U tensorflow-addons\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import mat73\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn.metrics\n",
        "from sklearn.utils import shuffle \n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Conv3D, MaxPooling3D, BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gy1WNR5zPNmG"
      },
      "outputs": [],
      "source": [
        "# extracted features that we corrected\n",
        "mat_file = sio.loadmat('ExtractedFeatures_1s/data.mat')\n",
        "\n",
        "# load labels to check accuracy\n",
        "mat_file_labels = sio.loadmat('ExtractedFeatures_1s/label.mat')\n",
        "count = 0\n",
        "accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PGTVwBOzBvA6"
      },
      "outputs": [],
      "source": [
        "data = mat_file['data']\n",
        "labels = mat_file_labels['label']\n",
        "\n",
        "# replicate labels for the 45 total experiments\n",
        "labels = labels[0]\n",
        "labels_edited = np.empty(675) #for every subject*experiment*trial\n",
        "for i in range(0,45):\n",
        "  labels_edited[i*15:(i+1)*15] = labels\n",
        "\n",
        "data, labels_edited = shuffle(data, labels_edited)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2MZ0tjXTOPHB"
      },
      "outputs": [],
      "source": [
        "cumul = []\n",
        "for i in range(4): cumul.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-PuMrXlRgFS",
        "outputId": "abdab869-335b-43cb-c3c2-13b5b15fd645"
      },
      "outputs": [],
      "source": [
        "max = np.max(data)\n",
        "min = np.min(data)\n",
        "\n",
        "#Normalizing data\n",
        "data = data/max\n",
        "data = (data - np.mean(data))/np.std(data)\n",
        "\n",
        "#Cropping end time segments\n",
        "data = data[:,:,0:250,:]\n",
        "\n",
        "#Splitting Dataset into train, validation, test \n",
        "train_labels = labels_edited[0:550]\n",
        "test_labels = labels_edited[550:600]\n",
        "train_data = data[0:550]\n",
        "test_data = data[550:600]\n",
        "final_test = data[600:675]\n",
        "final_labels = labels_edited[600:675]\n",
        "cf_labels = np.where(final_labels== -1, 2, final_labels) #Label in the form for confusion matrix\n",
        "un, co = np.unique(cf_labels, return_counts=True)\n",
        "print(f'Unique: {un}, Counts: {co}')\n",
        "\n",
        "train_labels_reshaped = train_labels.reshape(-1,1) #Formatting for input to the CNN model\n",
        "test_labels_reshaped = test_labels.reshape(-1,1)\n",
        "final_labels_reshaped = final_labels.reshape(-1,1)\n",
        "\n",
        "train_labels_reshaped = to_categorical(train_labels_reshaped, 3) #One Hot Encoding\n",
        "test_labels_reshaped = to_categorical(test_labels_reshaped, 3)\n",
        "final_labels_reshaped = to_categorical(final_labels_reshaped, 3)\n",
        "\n",
        "rnn_train = train_data.reshape(550, 62, -1) #Formatting for RNN input\n",
        "rnn_test = test_data.reshape(50, 62, -1)\n",
        "rnn_train = np.transpose(rnn_train, (0,2,1))\n",
        "rnn_test = np.transpose(rnn_test, (0,2,1))\n",
        "print(train_data.shape, rnn_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "tLU2CkVgH6bq",
        "outputId": "c1fabc4d-be21-4fa1-9a3d-eba5cdd1b10d"
      },
      "outputs": [],
      "source": [
        "fig, a = plt.subplots(5, 1, squeeze= False, figsize= (15, 10))\n",
        "titles = ['Alpha', 'Beta', 'Delta', 'Gamma', 'Theta']\n",
        "for cnt in range(5):\n",
        "  for i in range(len(test_data[5])):\n",
        "    a[cnt][0].plot(range(len(test_data[5][i,:,cnt])), test_data[5][i,:,cnt])\n",
        "  a[cnt][0].set_title(titles[cnt])\n",
        "  a[cnt][0].set_xlabel('Samples')\n",
        "  a[cnt][0].set_ylabel('Amplitude')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYoIqA8r5h1l"
      },
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_GPwXD_3tHy"
      },
      "source": [
        "#### CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtZ1OV39dCnb",
        "outputId": "365831d9-ea80-46cd-9649-a4ab880b75ba"
      },
      "outputs": [],
      "source": [
        "#Defining our CNN Model \n",
        "\n",
        "model = Sequential([\n",
        "  Conv2D(filters= 64, kernel_size= 5, input_shape = (62,250,5), padding= 'same'), \n",
        "  Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "  Conv2D(filters= 64, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2,strides = 2),\n",
        "  \n",
        "  Dropout(rate= 0.3),\n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'), \n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides = 2),\n",
        "  \n",
        "  Dropout(rate= 0.2),\n",
        "\n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides = 2),\n",
        "  \n",
        "  Dropout(rate= 0.25),\n",
        "\n",
        "  Conv2D(filters= 512, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides= 2),\n",
        "  \n",
        "  Dropout(rate= 0.3),\n",
        "  \n",
        "  Flatten(),\n",
        "\n",
        "  Dense(512, activation= 'relu'),\n",
        "  \n",
        "  Dropout(rate= 0.4),\n",
        "  Dense(256, activation= 'relu'),\n",
        "  \n",
        "  Dropout(rate= 0.2),\n",
        "  Dense(64, activation= 'relu'),\n",
        "  \n",
        "  # softmax activation, convert final layer to probability distributions of the 3 emotional categories \n",
        "  Dense(3, activation= 'softmax')\n",
        "])\n",
        "\n",
        "# apply optimizer and categorical cross entropy loss function. measure accuracy of predictions, uses RMSE for insight on error magnitude\n",
        "model.compile(optimizer= keras.optimizers.Adam(learning_rate=9e-5), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXR6yIkP3yQT"
      },
      "source": [
        "#### RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAO-_lk-6sY2",
        "outputId": "6bd1a88d-1a84-4104-dcc4-47e5afeacaf2"
      },
      "outputs": [],
      "source": [
        "#Defining our RNN model \n",
        "\n",
        "model_rnn = Sequential([\n",
        "  LSTM(units= 64, activation= 'tanh', input_shape= [1250, 62], return_sequences = True),\n",
        "  Dropout(0.25),\n",
        "  LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "  Dropout(0.3),\n",
        "  LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "  Dropout(0.35),\n",
        "  LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "  Flatten(),\n",
        "\n",
        "  Dense(256, activation= 'relu'),\n",
        "  Dropout(0.25),\n",
        "  Dense(128, activation= 'relu'),\n",
        "  Dropout(0.35),\n",
        "  Dense(64, activation= 'relu'),\n",
        "  Dense(3, activation= 'softmax'),\n",
        "\n",
        "])\n",
        "\n",
        "model_rnn.compile(optimizer= keras.optimizers.Adam(learning_rate=5e-5), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "model_rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-6lf5XI33cn"
      },
      "source": [
        "#### Hybrid Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ere0cwdP0Dy",
        "outputId": "82a0a424-e142-459b-b8ee-aa12191760ed"
      },
      "outputs": [],
      "source": [
        "#Defining our Hybrid model \n",
        "\n",
        "model_hybrid = Sequential([\n",
        "  Conv2D(filters= 64, kernel_size= 3, input_shape = (62,250,5), padding= 'same'), \n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2,strides = 2),\n",
        "  BatchNormalization(),\n",
        "\n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'), \n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides = 2),\n",
        "  BatchNormalization(),\n",
        "\n",
        "  Conv2D(filters= 512, kernel_size= 3, padding= 'same'), \n",
        "  BatchNormalization(),\n",
        "\n",
        "  Reshape((62, 15*512), input_shape= (15, 62, 512)), #Important to reshape so data passed to LSTM Layer correctly\n",
        "  \n",
        "  LSTM(units= 128, activation= 'tanh', return_sequences = True),\n",
        "  BatchNormalization(),\n",
        "  LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "  BatchNormalization(),\n",
        "  LSTM(units= 256, activation= 'tanh', return_sequences = True),\n",
        "  BatchNormalization(),\n",
        "  LSTM(units= 512, activation= 'tanh', return_sequences = True),\n",
        "  BatchNormalization(),\n",
        "  \n",
        "  Flatten(),\n",
        "  \n",
        "  Dense(512, activation= 'relu'), \n",
        "  Dropout(0.25), \n",
        "  Dense(256, activation= 'relu'),\n",
        "  Dropout(0.2),\n",
        "  Dense(64, activation= 'relu'),\n",
        "  Dense(3, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model_hybrid.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
        "model_hybrid.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZztDn4A8Yk"
      },
      "source": [
        "#### ViT Classifier Modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QP0GXP5QA_vB"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3 #1e-3\n",
        "#weight_decay = 0.0001\n",
        "batch_size = 128 #128\n",
        "num_epochs = 150\n",
        "image_size = [15, 15]  # We'll resize input images to this size\n",
        "patch_size = 3  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size)\n",
        "projection_dim = 64\n",
        "num_heads = 8\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8 #8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
        "input_shape = (15, 15, 128)\n",
        "num_classes= 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "haxBMSJHNf-F"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBQ9MsYk7ybL",
        "outputId": "48df0e99-0f5c-4900-dc24-06cea8cac0f6"
      },
      "outputs": [],
      "source": [
        "conv_block = Sequential([\n",
        "  Conv2D(filters= 64, kernel_size= 3, input_shape = (62,250,5), padding= 'same', strides= (1,2)), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "  Conv2D(filters= 64, kernel_size= 3, padding= 'same', strides= (1,2)),\n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2,strides = 2),\n",
        "  #BatchNormalization(),\n",
        "  Dropout(rate= 0.3),\n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'),\n",
        "  Conv2D(filters= 512, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides = 2),\n",
        "  Dropout(rate= 0.2),\n",
        "\n",
        "])\n",
        "\n",
        "conv_block.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQnc_ns6r1La",
        "outputId": "82cc6345-52e1-4f78-ca23-3719149897af"
      },
      "outputs": [],
      "source": [
        "conv_block_trial = Sequential([\n",
        "  \n",
        "  Conv2D(filters= 128, kernel_size= 3, input_shape = (62,265,5), padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "  Conv2D(filters= 128, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2,strides = 2),\n",
        "  #BatchNormalization(),\n",
        "  Dropout(rate= 0.3),\n",
        "\n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'),\n",
        "  Conv2D(filters= 256, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides = 2),\n",
        "  Dropout(rate= 0.2),\n",
        "\n",
        "\n",
        "  Conv2D(filters= 512, kernel_size= 3, padding= 'same'), #kernel_regularizer=tf.keras.regularizers.l1(0.01), activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "  Conv2D(filters= 512, kernel_size= 3, padding= 'same'),\n",
        "  Conv2D(filters= 512, kernel_size= 3, padding= 'same'),\n",
        "  MaxPooling2D(pool_size= 2, strides = (1,2)),\n",
        "  Dropout(rate= 0.25),\n",
        "])\n",
        "\n",
        "conv_block_trial.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rtEjpHPMG2Ao"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape= (62, 250, 5))\n",
        "    features = conv_block(inputs)\n",
        "    # Augment data.\n",
        "    #augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(features)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    outputs = layers.Softmax()(logits)\n",
        "    # Create the Keras model.\n",
        "    model_vit = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model_vit\n",
        "\n",
        "def run_experiment(model_vit):\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    reduce_lr_exp = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=4, min_lr=1e-6)  \n",
        "\n",
        "    history = model_vit.fit(\n",
        "        x= train_data,\n",
        "        y= train_labels_reshaped,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback, reduce_lr_exp],\n",
        "    )\n",
        "\n",
        "    model_vit.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model_vit.evaluate(test_data, test_labels_reshaped)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZTW8ZLTz4ww",
        "outputId": "90f0b547-8f47-4103-b163-8836ba9eb95f"
      },
      "outputs": [],
      "source": [
        "vit_classifier = create_vit_classifier()\n",
        "vit_classifier.summary()\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=learning_rate #, weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "vit_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "    ],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CivG4hMR39rd"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWxsDqXH4CXg"
      },
      "source": [
        "#### CNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlnvmTtPfxu3",
        "outputId": "40240396-2649-43af-9a3d-944c1e0f5dfd"
      },
      "outputs": [],
      "source": [
        "#Training our CNN model using a learning rate annealer (Reduces LR on Plateaus)\n",
        "reduce_lr_cnn = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-7) #patience = 5 and factor = 0.9\n",
        "\n",
        "history = model.fit(\n",
        "  train_data,\n",
        "  train_labels_reshaped,\n",
        "  batch_size = 64,\n",
        "  epochs=100, \n",
        "  validation_data=(test_data, test_labels_reshaped),\n",
        "  callbacks = [reduce_lr_cnn]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSMSxiHz4F-W"
      },
      "source": [
        "#### LSTM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "oNY9Z9cnCUj0",
        "outputId": "53843fbc-0e60-4c99-aa00-5e97d2049446"
      },
      "outputs": [],
      "source": [
        "#Training our RNN model using a learning rate annealer (Reduces LR on Plateaus)\n",
        "\n",
        "'''reduce_lr_rnn = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "history_rnn = model_rnn.fit(\n",
        "  rnn_train,\n",
        "  train_labels_reshaped,\n",
        "  batch_size = 64,\n",
        "  epochs=60,\n",
        "  validation_data=(rnn_test, test_labels_reshaped),\n",
        "  callbacks = [reduce_lr_rnn]\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEmLKU354KHa"
      },
      "source": [
        "#### Hybrid Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "vzLNinymUw3R",
        "outputId": "bef6d8db-7591-4b44-f3c3-4dc32c9c12ed"
      },
      "outputs": [],
      "source": [
        "#Training our Hybrid model using a learning rate annealer (Reduces LR on Plateaus)\n",
        "\n",
        "'''reduce_lr_hybrid = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "history_hybrid = model_hybrid.fit(\n",
        "  train_data,\n",
        "  train_labels_reshaped,\n",
        "  batch_size = 32,\n",
        "  epochs=60,\n",
        "  validation_data=(test_data, test_labels_reshaped),\n",
        "  callbacks = [reduce_lr_hybrid]\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQRD1-9WYDuO"
      },
      "source": [
        "#### ViT Modified Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "c0mEudkkSOaW",
        "outputId": "16112c7e-4e15-4fb5-9d81-26e5d8596ab6"
      },
      "outputs": [],
      "source": [
        "'''history_exp = run_experiment(vit_classifier)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rid7DGDU9lMe"
      },
      "source": [
        "## Saving and Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX-0sd2i9prU"
      },
      "outputs": [],
      "source": [
        "#NOTE: Loading pretrained models should only be done when the same data distribution (as shuffle is on) is also loaded otherwise you will get inflated results\n",
        "model.load_weights(\"/content/drive/MyDrive/Colab Notebooks/cnn_best.h5\")\n",
        "\n",
        "'''model_rnn.load_weights(\"/content/drive/MyDrive/Colab Notebooks/rnn_best.h5\")\n",
        "model_hybrid.load_weights(\"/content/drive/MyDrive/Colab Notebooks/hybrid_best.h5\")\n",
        "vit_classifier.load_weights(\"/content/drive/MyDrive/Colab Notebooks/vit_best.h5\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QozTtxiq4POU"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3uRq7xDYb-S"
      },
      "source": [
        "#### Diagrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L93Th_aUXAK9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = len(history.history['loss'])\n",
        "epochs_rnn = len(history_rnn.history['accuracy'])\n",
        "epochs_hybrid = len(history_hybrid.history['accuracy'])\n",
        "epochs_exp = len(history_exp.history['accuracy'])\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (30,10)\n",
        "fig, a = plt.subplots(3, 4, squeeze= False)\n",
        "\n",
        "#plt.figure(1)\n",
        "a[0][0].plot(range(0,epochs), history.history['val_accuracy'], 'r', label = 'Test Accuracy')\n",
        "a[0][0].plot(range(0,epochs), history.history['accuracy'], 'y', label = 'Train Accuracy')\n",
        "a[0][0].set_title('Accuracy CNN')\n",
        "a[0][0].set_xlabel('Epochs')\n",
        "a[0][0].set_ylabel('Accuracy')\n",
        "a[0][0].legend()\n",
        "#plt.show()\n",
        "\n",
        "'''\n",
        "#plt.figure(2)\n",
        "a[0][1].plot(range(0,epochs_rnn), history_rnn.history['val_accuracy'], 'r', label = 'Test Accuracy')\n",
        "a[0][1].plot(range(0,epochs_rnn), history_rnn.history['accuracy'], 'y', label = 'Train Accuracy')\n",
        "a[0][1].set_title('Accuracy RNN')\n",
        "a[0][1].set_xlabel('Epochs')\n",
        "a[0][1].set_ylabel('Accuracy')\n",
        "a[0][1].legend()\n",
        "\n",
        "#plt.figure(3)\n",
        "a[0][2].plot(range(0,epochs_hybrid), history_hybrid.history['val_accuracy'], 'r', label = 'Test Accuracy')\n",
        "a[0][2].plot(range(0,epochs_hybrid), history_hybrid.history['accuracy'], 'y', label = 'Train Accuracy')\n",
        "a[0][2].set_title('Accuracy Hybrid')\n",
        "a[0][2].set_xlabel('Epochs')\n",
        "a[0][2].set_ylabel('Accuracy')\n",
        "a[0][2].legend()\n",
        "\n",
        "a[0][3].plot(range(0,epochs_exp), history_exp.history['val_accuracy'], 'r', label = 'Test Accuracy')\n",
        "a[0][3].plot(range(0,epochs_exp), history_exp.history['accuracy'], 'y', label = 'Train Accuracy')\n",
        "a[0][3].set_title('Accuracy ViT')\n",
        "a[0][3].set_xlabel('Epochs')\n",
        "a[0][3].set_ylabel('Accuracy')\n",
        "a[0][3].legend()\n",
        "'''\n",
        "\n",
        "#plt.figure(1)\n",
        "a[1][0].plot(range(0,epochs - 40), history.history['val_loss'][0:60], 'r', label = 'Test Loss')\n",
        "a[1][0].plot(range(0,epochs - 40), history.history['loss'][0:60], 'y', label = 'Train Loss')\n",
        "a[1][0].set_title('Loss CNN')\n",
        "a[1][0].set_xlabel('Epochs')\n",
        "a[1][0].set_ylabel('Loss')\n",
        "a[1][0].legend()\n",
        "#plt.show()\n",
        "\n",
        "'''\n",
        "#plt.figure(2)\n",
        "a[1][1].plot(range(0,epochs_rnn), history_rnn.history['val_loss'], 'r', label = 'Test Loss')\n",
        "a[1][1].plot(range(0,epochs_rnn), history_rnn.history['loss'], 'y', label = 'Train Loss')\n",
        "a[1][1].set_title('Loss RNN')\n",
        "a[1][1].set_xlabel('Epochs')\n",
        "a[1][1].set_ylabel('Loss')\n",
        "a[1][1].legend()\n",
        "\n",
        "#plt.figure(3)\n",
        "a[1][2].plot(range(0,epochs_hybrid), history_hybrid.history['val_loss'], 'r', label = 'Test Loss')\n",
        "a[1][2].plot(range(0,epochs_hybrid), history_hybrid.history['loss'], 'y', label = 'Train Loss')\n",
        "a[1][2].set_title('Loss Hybrid')\n",
        "a[1][2].set_xlabel('Epochs')\n",
        "a[1][2].set_ylabel('Loss')\n",
        "a[1][2].legend()\n",
        "\n",
        "a[1][3].plot(range(0,epochs_exp), history_exp.history['val_loss'], 'r', label = 'Test Loss')\n",
        "a[1][3].plot(range(0,epochs_exp), history_exp.history['loss'], 'y', label = 'Train Loss')\n",
        "a[1][3].set_title('Loss ViT')\n",
        "a[1][3].set_xlabel('Epochs')\n",
        "a[1][3].set_ylabel('Loss')\n",
        "a[1][3].legend()\n",
        "'''\n",
        "\n",
        "#plt.figure(1)\n",
        "a[2][0].plot(range(0,epochs - 40), history.history['lr'][0:60], 'r', label = 'Test Learning rate')\n",
        "a[2][0].set_title('Learning rate CNN')\n",
        "a[2][0].set_xlabel('Epochs')\n",
        "a[2][0].set_ylabel('Learning rate')\n",
        "a[2][0].legend()\n",
        "#plt.show()\n",
        "\n",
        "'''\n",
        "#plt.figure(2)\n",
        "a[2][1].plot(range(0,epochs_rnn), history_rnn.history['lr'], 'r', label = 'Test Learning rate')\n",
        "a[2][1].set_title('Learning rate RNN')\n",
        "a[2][1].set_xlabel('Epochs')\n",
        "a[2][1].set_ylabel('Learning rate')\n",
        "a[2][1].legend()\n",
        "\n",
        "#plt.figure(3)\n",
        "a[2][2].plot(range(0,epochs_hybrid), history_hybrid.history['lr'], 'r', label = 'Test Learning rate')\n",
        "a[2][2].set_title('Learning rate Hybrid')\n",
        "a[2][2].set_xlabel('Epochs')\n",
        "a[2][2].set_ylabel('Learning rate')\n",
        "a[2][2].legend()\n",
        "\n",
        "a[2][3].plot(range(0,epochs_exp), history_exp.history['lr'], 'r', label = 'Test Learning rate')\n",
        "a[2][3].set_title('Learning rate ViT')\n",
        "a[2][3].set_xlabel('Epochs')\n",
        "a[2][3].set_ylabel('Learning rate')\n",
        "a[2][3].legend()\n",
        "#plt.savefig('LR.eps', format='eps')\n",
        "'''\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jvm74kRYewB"
      },
      "source": [
        "#### Metrics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTJHOO5MXVlt",
        "outputId": "ed1a410b-2cf9-4d8e-f04b-d2fc14ebba96"
      },
      "outputs": [],
      "source": [
        "#Calculating Metrics, Confusion Matrices and Classification Reports\n",
        "his = model.evaluate(x= final_test, y = final_labels_reshaped)\n",
        "accuracy = his[1]\n",
        "final_preds = np.empty([3, 75])\n",
        "pred_labels = np.argmax(model.predict(final_test), axis=-1)\n",
        "\n",
        "'''\n",
        "final_rnn_test = final_test.reshape(75, 62, -1)\n",
        "final_rnn_test = np.transpose(final_rnn_test, (0,2,1))\n",
        "\n",
        "his_rnn = model_rnn.evaluate(x= final_rnn_test, y = final_labels_reshaped)\n",
        "accuracy_rnn = his_rnn[1]\n",
        "pred_labels_rnn = np.argmax(model_rnn.predict(final_rnn_test), axis=-1)\n",
        "\n",
        "\n",
        "his_hybrid = model_hybrid.evaluate(x= final_test, y = final_labels_reshaped)\n",
        "accuracy_hybrid = his_hybrid[1]\n",
        "pred_labels_hybrid = np.argmax(model_hybrid.predict(final_test), axis=-1)\n",
        "\n",
        "his_exp = vit_classifier.evaluate(x= final_test, y= final_labels_reshaped)\n",
        "accuracy_exp = his_exp[1]\n",
        "pred_labels_exp = np.argmax(vit_classifier.predict(final_test), axis=-1)\n",
        "'''\n",
        "\n",
        "final_preds[0,:] = pred_labels\n",
        "'''\n",
        "final_preds[1,:] = pred_labels_rnn\n",
        "final_preds[2,:] = pred_labels_hybrid\n",
        "#final_preds[1,:] = pred_labels_exp\n",
        "'''\n",
        "\n",
        "#Implementing Ensemble\n",
        "\n",
        "'''ensemble_predictions = []\n",
        "itr = 0\n",
        "for i in range (0,75):\n",
        "  itr += 1\n",
        "  unique, counts = np.unique(final_preds[:,i], return_counts=True)\n",
        "  if len(unique) == 3:\n",
        "    ensemble_predictions.append(int(final_preds[1,i]))\n",
        "  else:\n",
        "    max = np.max(counts)\n",
        "    index = np.unravel_index(np.argmax(counts), counts.shape)\n",
        "    pred = unique[index]\n",
        "    ensemble_predictions.append(int(pred))\n",
        "\n",
        "accuracy_ensemble = sklearn.metrics.accuracy_score(cf_labels, ensemble_predictions)\n",
        "'''\n",
        "\n",
        "cf_cnn = sklearn.metrics.confusion_matrix(cf_labels, pred_labels, labels = [2,0,1])\n",
        "'''cf_rnn = sklearn.metrics.confusion_matrix(cf_labels, pred_labels_rnn, labels = [2,0,1])\n",
        "cf_hybrid = sklearn.metrics.confusion_matrix(cf_labels, pred_labels_hybrid, labels = [2,0,1])\n",
        "cf_exp = sklearn.metrics.confusion_matrix(cf_labels, pred_labels_exp, labels = [2,0,1])\n",
        "cf_ensemble = sklearn.metrics.confusion_matrix(cf_labels, ensemble_predictions, labels = [2,0,1])\n",
        "cf = sklearn.metrics.confusion_matrix(cf_labels, pred_labels, labels= [2,0,1])\n",
        "'''\n",
        "cr = sklearn.metrics.classification_report(cf_labels, pred_labels, labels= [2,0,1])\n",
        "'''cr_rnn = sklearn.metrics.classification_report(cf_labels, pred_labels_rnn, labels= [2,0,1])\n",
        "cr_hybrid = sklearn.metrics.classification_report(cf_labels, pred_labels_hybrid, labels= [2,0,1])\n",
        "cr_exp = sklearn.metrics.classification_report(cf_labels, pred_labels_exp, labels = [2,0,1])\n",
        "cr_ensemble = sklearn.metrics.classification_report(cf_labels, ensemble_predictions, labels= [2,0,1])\n",
        "'''\n",
        "mcc = sklearn.metrics.matthews_corrcoef(cf_labels, pred_labels)\n",
        "'''mcc_rnn = sklearn.metrics.matthews_corrcoef(cf_labels, pred_labels_rnn)\n",
        "mcc_hybrid = sklearn.metrics.matthews_corrcoef(cf_labels, pred_labels_hybrid)\n",
        "mcc_ensemble = sklearn.metrics.matthews_corrcoef(cf_labels, ensemble_predictions) \n",
        "mcc_exp = sklearn.metrics.matthews_corrcoef(cf_labels, pred_labels_exp)\n",
        "'''\n",
        "\n",
        "print(\"\\n\", \"Accuracy of CNN: \" + str(accuracy))\n",
        "print(f'MCC CNN: {mcc}\\n')\n",
        "\n",
        "'''print(\"\\n\", \"Accuracy of CNN: \" + str(accuracy), \"Accuracy of RNN: \" + str(accuracy_rnn), \"Accuracy of Hybrid Model: \" + str(accuracy_hybrid), \"Accuracy ViT: \" + str(accuracy_exp), \"Accuracy Ensemble: \" + str(accuracy_ensemble), \"\\n\", \"Confusion Matrix CNN: \", str(cf), \"Confusion Matrix RNN: \", str(cf_rnn), \"Confusion Matrix Hybrid: \", str(cf_hybrid), \"Confusion Matrix ViT: \", str(cf_exp), \"Confusion Matrix Ensemble: \", str(cf_ensemble), \"\\n\", \"Classification Report CNN: \", str(cr), \"Classification Report RNN: \", str(cr_rnn),  \"Classification Report Hybrid: \", str(cr_hybrid), \"Classification Report ViT: \", str(cr_exp), \"Classification Report Ensemble: \", str(cr_ensemble), sep= '\\n')\n",
        "print(f'MCC CNN: {mcc}\\n MCC RNN: {mcc_rnn}\\n MCC Hybrid: {mcc_hybrid}\\n MCC ViT: {mcc_exp}\\n MCC Ensemble: {mcc_ensemble}')\n",
        "'''\n",
        "\n",
        "#Use below code to save model and weights permanently\n",
        "\n",
        "\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/best_cnn_data.npy', 'wb') as f:\n",
        "#     np.save(f, data)\n",
        "#     np.save(f, train_data)\n",
        "#     np.save(f, test_data)\n",
        "#     np.save(f, final_test)\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/best_cnn_data.npy', 'rb') as f:\n",
        "#     a = np.load(f)\n",
        "#     b = np.load(f)\n",
        "#     c = np.load(f)\n",
        "#     d = np.load(f)\n",
        "# print(np.testing.assert_allclose(d, final_test, err_msg= \"FAIL\"))\n",
        "\n",
        "# model_json = model.to_json()\n",
        "# with open(\"/content/drive/MyDrive/Colab Notebooks/cnn_best.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# # serialize weights to HDF5\n",
        "# model.save_weights(\"/content/drive/MyDrive/Colab Notebooks/cnn_best.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "\n",
        "# # load json and create model\n",
        "# json_file = open('/content/drive/MyDrive/Colab Notebooks/ensemble_best.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "# # load weights into new model\n",
        "# loaded_model.load_weights(\"/content/drive/MyDrive/Colab Notebooks/ensemble_best.h5\")\n",
        "# print(\"Loaded model from disk\")\n",
        "\n",
        "# loaded_model.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= ['accuracy', 'categorical_accuracy', tf.keras.metrics.RootMeanSquaredError()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZVu9UgQ5MPl"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM884CsvnjW-"
      },
      "outputs": [],
      "source": [
        "'''pred = np.argmax(model.predict(train_data), axis=-1)\n",
        "pred_rnn = np.argmax(model_rnn.predict(rnn_train), axis=-1)\n",
        "pred_hybrid = np.argmax(model_hybrid.predict(train_data), axis=-1)\n",
        "inputs = [pred, pred_rnn, pred_hybrid]\n",
        "inputs = np.array(inputs).T\n",
        "\n",
        "pred = np.argmax(model.predict(test_data), axis=-1)\n",
        "pred_rnn = np.argmax(model_rnn.predict(rnn_test), axis=-1)\n",
        "pred_hybrid = np.argmax(model_hybrid.predict(test_data), axis=-1)\n",
        "inputs_test = [pred, pred_rnn, pred_hybrid]\n",
        "inputs_test = np.array(inputs_test).T\n",
        "\n",
        "model_stack = Sequential([\n",
        "  #Input(shape= (3, 550)),\n",
        "  Dense(128, 'relu', input_shape= (3,)),\n",
        "  Dense(256, 'relu'),\n",
        "  Dense(256, 'relu'),\n",
        "  Dense(64, 'relu'),\n",
        "  Dense(3, 'softmax')\n",
        "])\n",
        "\n",
        "model_stack.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-4), loss= keras.losses.categorical_crossentropy, metrics= 'accuracy')\n",
        "\n",
        "model_stack.summary()\n",
        "\n",
        "reduce_lr_stack = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #patience = 5 and factor = 0.9\n",
        "\n",
        "history_stack = model_stack.fit(\n",
        "  inputs,\n",
        "  train_labels_reshaped,\n",
        "  batch_size = 32,\n",
        "  epochs=150,\n",
        "  validation_data=(inputs_test, test_labels_reshaped),\n",
        "  callbacks = [reduce_lr_stack]\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfnZAUvJ3WGD"
      },
      "outputs": [],
      "source": [
        "'''acc = model_stack.evaluate(x= final_preds.T, y= final_labels_reshaped)\n",
        "print(f'Accuracy Ensemble MAX method: {accuracy_ensemble}, Accuracy Ensemble Stack method: {acc[1]}')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbs4WZW4PsEp"
      },
      "outputs": [],
      "source": [
        "'''cumul[0].append(accuracy)\n",
        "cumul[1].append(accuracy_rnn)\n",
        "cumul[2].append(accuracy_hybrid)\n",
        "cumul[3].append(accuracy_ensemble)\n",
        "cumul[4].append(acc[1])\n",
        "cumul\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iTtPm17dvkm"
      },
      "outputs": [],
      "source": [
        "#model_rnn.save('saved_model/rnn_model')\n",
        "#model_hybrid.save('saved_model/hybrid_model')\n",
        "#!zip -r /content/saved_model.zip /content/saved_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMDcSRnOikfUITevQIj8ASO",
      "collapsed_sections": [
        "FyIT6Er83eqi",
        "k_GPwXD_3tHy",
        "RXR6yIkP3yQT",
        "g-6lf5XI33cn",
        "MGZztDn4A8Yk",
        "wWxsDqXH4CXg",
        "zSMSxiHz4F-W",
        "yEmLKU354KHa",
        "mQRD1-9WYDuO",
        "U3uRq7xDYb-S"
      ],
      "mount_file_id": "1sXb_M7i01_pkD4lPouGwYCUh9MkcnEPj",
      "name": "Github_EEG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ppbEmo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
